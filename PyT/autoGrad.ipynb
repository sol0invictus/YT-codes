{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGrad in PyTorch\n",
    "\n",
    "In this notebook we will review the automatic differentaion mechanics in PyTorch. We will see the workings of autograd for both Scalar and Non-Scalar differentiaions.\n",
    "\n",
    "Video: https://youtu.be/I2MRiWPeXDY\n",
    "\n",
    "Ref:\n",
    "1) https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "2) https://pytorch.org/docs/stable/autograd.html\n",
    "3) https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n",
    "4) https://pytorch.org/docs/stable/notes/autograd.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first begin by importing the reqired libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalar Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me first define a function that takes a scalar (a) and computed 5*a^2 of it. We return both the ouput and the initial value. The reason for creating a function to do this trivial task will be clear later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute():\n",
    "    a = torch.tensor([3.], requires_grad=True)\n",
    "    O = 5*a**2\n",
    "    return [a,O]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we our function, lets print the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.], requires_grad=True)\n",
      "tensor([45.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "[a,O] = compute()\n",
    "print(a)\n",
    "print(O)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform differentiation. The is done by invoking \".backward()\" on the output scalar. Once this is done all variables with (\"required_grad=True\") flag will have their gradients computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([30.])\n"
     ]
    }
   ],
   "source": [
    "O.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following lines we will recompute the same without a function. When we call the function multiple times, it overwrites 'a' and 'O\" with new compute graphs. However if I do not create a function can perform computation multiple times, it uses the same graph and accumulates the gradients. After every iteration the gradients need to be zeroed out (refresh the graph) for proper computations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([2.], requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20.])\n"
     ]
    }
   ],
   "source": [
    "O = 5*a**2\n",
    "O.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us inspect the auto-diff graph. For our computation the graph looks like the following:\n",
    "\n",
    "![Compute Graph](compGraph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MulBackward0 object at 0x0000022027A86E80>\n",
      "((<PowBackward0 object at 0x0000022027A86310>, 0), (None, 0))\n"
     ]
    }
   ],
   "source": [
    "[a,O] = compute()\n",
    "print(O.grad_fn)\n",
    "print(O.grad_fn.next_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manully call backward functions to compute the values that they return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([5.]), None)\n",
      "tensor([6.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "temp = torch.tensor([1.])\n",
    "print(O.grad_fn(temp))\n",
    "print(O.grad_fn.next_functions[0][0](temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Scalar Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we saw scalar case, what about matrix derivates (Neural Networks). To demonstrate the process we will create a small neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = torch.ones(3)  # input tensor\n",
    "w = torch.randn(3, 2, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "y = torch.zeros(2) # Output\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4273, 0.1037],\n",
      "        [0.4273, 0.1037],\n",
      "        [0.4273, 0.1037]])\n",
      "tensor([0.4273, 0.1037])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No-Grad mode\n",
    "\n",
    "Generating computational graph and keeping track of gradients hurts both performance and memory. For inference, computation graphs are not required and we can perform computation by dsiabling gradient tracking. This can be done in the following ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
